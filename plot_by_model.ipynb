{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import os\n",
    "path = \"/cluster/home/mlindner/Github/master_thesis_project/src/data/outputs\"\n",
    "plots_path = \"/cluster/home/mlindner/Github/master_thesis_project/src/data/outputs/plots/by_model/\"\n",
    "os.makedirs(plots_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "file_name = \"250417.csv\"\n",
    "df = pd.read_csv(os.path.join(path, file_name))\n",
    "\n",
    "model_names = [\n",
    "    \"deepseek-chat\", \n",
    "    \"claude-3-haiku-20240307\", \n",
    "    \"gemini-2.0-flash-lite\", \n",
    "    #\"gemini-1.5-flash-002\", \n",
    "    \"gpt-4o-mini-2024-07-18\", \n",
    "    #\"gemini-2.0-flash\", \n",
    "    \"gemini-2.5-pro-exp-03-25\", \n",
    "    \"gpt-4.1-mini-2025-04-14\", \n",
    "    \"gpt-3.5-turbo\"\n",
    "]\n",
    "# Filter the DataFrame to include only the specified model names\n",
    "df = df[df['model_name_1'].isin(model_names) | df['model_name_2'].isin(model_names)]\n",
    "df['agent_1_scores'] = df['agent_1_scores'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['agent_2_scores'] = df['agent_2_scores'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['agent_1_cumulative_scores'] = df['agent_1_scores'].apply(lambda x: [0] + [sum(x[:i+1]) for i in range(len(x))] if isinstance(x, list) else x) \n",
    "df['agent_2_cumulative_scores'] = df['agent_2_scores'].apply(lambda x: [0] + [sum(x[:i+1]) for i in range(len(x))] if isinstance(x, list) else x)\n",
    "\n",
    "#parse as lists, something went wrong when saving the csv\n",
    "df['truthful_agent_1'] = df['truthful_agent_1'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['truthful_agent_2'] = df['truthful_agent_2'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "# Parse the actions as lists of strings\n",
    "df['agent_1_actions'] = df['agent_1_actions'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['agent_2_actions'] = df['agent_2_actions'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['agent_1_messages'] = df['agent_1_messages'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['agent_2_messages'] = df['agent_2_messages'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['intent_agent_1'] = df['intent_agent_1'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['intent_agent_2'] = df['intent_agent_2'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df = df.rename(columns={'game_name': 'GameName'})\n",
    "\n",
    "# Create a new DataFrame with one row per agent\n",
    "df_agent1 = df.copy()\n",
    "df_agent2 = df.copy()\n",
    "\n",
    "# Add a column to indicate the agent\n",
    "df_agent1['Agent'] = 'Agent 1'\n",
    "df_agent2['Agent'] = 'Agent 2'\n",
    "# Rename columns to remove agent-specific prefixes\n",
    "df_agent1 = df_agent1.rename(columns={\n",
    "    'personality_1': 'Personality',\n",
    "    'agent_1_scores': 'Scores',\n",
    "    'agent_1_messages': 'Messages',\n",
    "    'agent_1_actions': 'Actions',\n",
    "    'agent_1_cumulative_scores': 'CumulativeScores',\n",
    "    'intent_agent_1': 'Intent',\n",
    "    'truthful_agent_1': 'Truthful',\n",
    "    'analysis_agent_1': 'Analysis',\n",
    "    'model_name_1': 'ModelName',\n",
    "    'model_provider_1': 'ModelProvider',\n",
    "})\n",
    "\n",
    "df_agent2 = df_agent2.rename(columns={\n",
    "    'personality_2': 'Personality',\n",
    "    'agent_2_scores': 'Scores',\n",
    "    'agent_2_messages': 'Messages',\n",
    "    'agent_2_actions': 'Actions',\n",
    "    'agent_2_cumulative_scores': 'CumulativeScores',\n",
    "    'intent_agent_2': 'Intent',\n",
    "    'truthful_agent_2': 'Truthful',\n",
    "    'analysis_agent_2': 'Analysis',\n",
    "    'model_name_2': 'ModelName',\n",
    "    'model_provider_2': 'ModelProvider',\n",
    "})\n",
    "\n",
    "# Remove the other columns\n",
    "df_agent1 = df_agent1.drop(columns=[\n",
    "    'personality_2', 'agent_2_scores', 'agent_2_messages', 'agent_2_actions', \n",
    "    'agent_2_cumulative_scores', 'intent_agent_2', 'truthful_agent_2', 'analysis_agent_2',\n",
    "    'model_name_2', 'model_provider_2'\n",
    "])\n",
    "df_agent2 = df_agent2.drop(columns=[\n",
    "    'personality_1', 'agent_1_scores', 'agent_1_messages', 'agent_1_actions', \n",
    "    'agent_1_cumulative_scores', 'intent_agent_1', 'truthful_agent_1', 'analysis_agent_1',\n",
    "    'model_name_1', 'model_provider_1'\n",
    "])\n",
    "\n",
    "# Combine the two DataFrames\n",
    "df_agents = pd.concat([df_agent1, df_agent2], ignore_index=True)\n",
    "\n",
    "nonmbti = ['NONE', 'ALTRUISTIC', 'SELFISH']\n",
    "# Map each personality to its dichotomies\n",
    "df_agents['I/E'] = df_agents['Personality'].apply(lambda x: 'I' if x[0] == 'I' else 'E' if x not in nonmbti else None)\n",
    "df_agents['N/S'] = df_agents['Personality'].apply(lambda x: 'N' if x[1] == 'N' else 'S' if x not in nonmbti else None)\n",
    "df_agents['T/F'] = df_agents['Personality'].apply(lambda x: 'T' if x[2] == 'T' else 'F' if x not in nonmbti else None)\n",
    "df_agents['J/P'] = df_agents['Personality'].apply(lambda x: 'J' if x[3] == 'J' else 'P' if x not in nonmbti else None)\n",
    "\n",
    "df_agents[\"TotalScore\"] = df_agents[\"CumulativeScores\"].apply(lambda x: x[-1])\n",
    "df_agents[\"Truthfullness\"] = df_agents[\"Truthful\"].apply(lambda x: sum(x)/len(x) if len(x) > 0 else 0)\n",
    "\n",
    "#total cost USD\n",
    "print(df_agents['total_cost_USD'].sum())\n",
    "\n",
    "df_agents.head()\n",
    "#df_agents.to_csv(\"/cluster/home/mlindner/Github/master_thesis_project/src/data/outputs/plots/stability_PD/stability_PD.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_palette(\"husl\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "sns.violinplot(x='I/E', y='TotalScore', data=df_agents, ax=axs[0, 0])\n",
    "sns.violinplot(x='N/S', y='TotalScore', data=df_agents, ax=axs[0, 1])\n",
    "sns.violinplot(x='T/F', y='TotalScore', data=df_agents, ax=axs[1, 0])\n",
    "sns.violinplot(x='J/P', y='TotalScore', data=df_agents, ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_path + \"score_violin_per_dichotomy.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Prepare the data for plotting\n",
    "df_agents['Round'] = df_agents['CumulativeScores'].apply(lambda x: list(range(len(x))))\n",
    "df_agents['TruthfulByRound'] = df_agents.apply(lambda row: list(zip(row['Round'], row['Truthful'])), axis=1)\n",
    "\n",
    "# Explode the data to have one row per round\n",
    "df_exploded = df_agents.explode('TruthfulByRound')\n",
    "df_exploded['Round'] = df_exploded['TruthfulByRound'].apply(lambda x: x[0] if isinstance(x, tuple) else None)\n",
    "df_exploded['Truthful'] = df_exploded['TruthfulByRound'].apply(lambda x: x[1] if isinstance(x, tuple) else None)\n",
    "\n",
    "# Filter out rows with missing values\n",
    "df_exploded = df_exploded.dropna(subset=['Round', 'Truthful'])\n",
    "\n",
    "# Convert Round to integer for proper sorting\n",
    "df_exploded['Round'] = df_exploded['Round'].astype(int)\n",
    "\n",
    "# Plot average truthfulness by round for each dichotomy\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_palette(\"husl\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "sns.lineplot(data=df_exploded, x='Round', y='Truthful', hue='I/E', ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Average Truthfulness by Round - I/E')\n",
    "\n",
    "sns.lineplot(data=df_exploded, x='Round', y='Truthful', hue='N/S', ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Average Truthfulness by Round - N/S')\n",
    "\n",
    "sns.lineplot(data=df_exploded, x='Round', y='Truthful', hue='T/F', ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Average Truthfulness by Round - T/F')\n",
    "\n",
    "sns.lineplot(data=df_exploded, x='Round', y='Truthful', hue='J/P', ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Average Truthfulness by Round - J/P')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_path + \"average_truthfulness_by_round_dichotomy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Prepare the data for plotting by model\n",
    "#sns.set(style=\"whitegrid\")\n",
    "#sns.set_context(\"talk\")\n",
    "#sns.set_palette(\"husl\")\n",
    "#\n",
    "#for model_name in model_names:\n",
    "#    model_data = df_agents[df_agents['ModelName'] == model_name]\n",
    "#    model_data['Round'] = model_data['CumulativeScores'].apply(lambda x: list(range(len(x))))\n",
    "#    model_data['TruthfulByRound'] = model_data.apply(lambda row: list(zip(row['Round'], row['Truthful'])), axis=1)\n",
    "#\n",
    "#    # Explode the data to have one row per round\n",
    "#    model_exploded = model_data.explode('TruthfulByRound')\n",
    "#    model_exploded['Round'] = model_exploded['TruthfulByRound'].apply(lambda x: x[0] if isinstance(x, tuple) else None)\n",
    "#    model_exploded['Truthful'] = model_exploded['TruthfulByRound'].apply(lambda x: x[1] if isinstance(x, tuple) else None)\n",
    "#\n",
    "#    # Filter out rows with missing values\n",
    "#    model_exploded = model_exploded.dropna(subset=['Round', 'Truthful'])\n",
    "#\n",
    "#    # Convert Round to integer for proper sorting\n",
    "#    model_exploded['Round'] = model_exploded['Round'].astype(int)\n",
    "#\n",
    "#    # Create subplots for each dichotomy\n",
    "#    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "#    fig.suptitle(f\"Average Truthfulness by Round for {model_name}\", fontsize=16)\n",
    "#\n",
    "#    sns.lineplot(data=model_exploded, x='Round', y='Truthful', hue='I/E', ax=axs[0, 0])\n",
    "#    axs[0, 0].set_title('I/E Dichotomy')\n",
    "#\n",
    "#    sns.lineplot(data=model_exploded, x='Round', y='Truthful', hue='N/S', ax=axs[0, 1])\n",
    "#    axs[0, 1].set_title('N/S Dichotomy')\n",
    "#\n",
    "#    sns.lineplot(data=model_exploded, x='Round', y='Truthful', hue='T/F', ax=axs[1, 0])\n",
    "#    axs[1, 0].set_title('T/F Dichotomy')\n",
    "#\n",
    "#    sns.lineplot(data=model_exploded, x='Round', y='Truthful', hue='J/P', ax=axs[1, 1])\n",
    "#    axs[1, 1].set_title('J/P Dichotomy')\n",
    "#\n",
    "#    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "#    plt.savefig(plots_path + f\"average_truthfulness_by_round_dichotomy_{model_name}.png\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create violin plots for Truthfullness across individual dichotomies\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_palette(\"husl\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "sns.violinplot(x='I/E', y='Truthfullness', data=df_agents, ax=axs[0, 0], hue='I/E')\n",
    "axs[0, 0].axhline(df_agents[df_agents['I/E'] == 'I']['Truthfullness'].mean(), color='blue', linestyle='--', label='Mean I')\n",
    "axs[0, 0].axhline(df_agents[df_agents['I/E'] == 'E']['Truthfullness'].mean(), color='orange', linestyle='--', label='Mean E')\n",
    "\n",
    "sns.violinplot(x='N/S', y='Truthfullness', data=df_agents, ax=axs[0, 1], hue='N/S')\n",
    "axs[0, 1].axhline(df_agents[df_agents['N/S'] == 'N']['Truthfullness'].mean(), color='blue', linestyle='--', label='Mean N')\n",
    "axs[0, 1].axhline(df_agents[df_agents['N/S'] == 'S']['Truthfullness'].mean(), color='orange', linestyle='--', label='Mean S')\n",
    "\n",
    "\n",
    "sns.violinplot(x='T/F', y='Truthfullness', data=df_agents, ax=axs[1, 0], hue='T/F')\n",
    "axs[1, 0].axhline(df_agents[df_agents['T/F'] == 'T']['Truthfullness'].mean(), color='blue', linestyle='--', label='Mean T')\n",
    "axs[1, 0].axhline(df_agents[df_agents['T/F'] == 'F']['Truthfullness'].mean(), color='orange', linestyle='--', label='Mean F')\n",
    "\n",
    "sns.violinplot(x='J/P', y='Truthfullness', data=df_agents, ax=axs[1, 1], hue='J/P')\n",
    "axs[1, 1].axhline(df_agents[df_agents['J/P'] == 'J']['Truthfullness'].mean(), color='blue', linestyle='--', label='Mean J')\n",
    "axs[1, 1].axhline(df_agents[df_agents['J/P'] == 'P']['Truthfullness'].mean(), color='orange', linestyle='--', label='Mean P')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_path + \"truthfullness_violin_per_dichotomy_all_models.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create individual plots for each game with dichotomies next to each other\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "model_names = df_agents['ModelName'].unique()\n",
    "\n",
    "for model_name in model_names:\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    model_data = df_agents[df_agents['ModelName'] == model_name]\n",
    "    \n",
    "    sns.violinplot(x='I/E', y='Truthfullness', data=model_data, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title(f'{model_name} - I/E')\n",
    "    \n",
    "    sns.violinplot(x='N/S', y='Truthfullness', data=model_data, ax=axs[0, 1])\n",
    "    axs[0, 1].set_title(f'{model_name} - N/S')\n",
    "    \n",
    "    sns.violinplot(x='T/F', y='Truthfullness', data=model_data, ax=axs[1, 0])\n",
    "    axs[1, 0].set_title(f'{model_name} - T/F')\n",
    "    \n",
    "    sns.violinplot(x='J/P', y='Truthfullness', data=model_data, ax=axs[1, 1])\n",
    "    axs[1, 1].set_title(f'{model_name} - J/P')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(plots_path + f\"truthfullness_violin_{model_name}_dichotomies.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more significant is that for each game the introverted lies less: some of the matrices or game description do make a difference in an absolute sense but relative, way less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "I_E = df_agents[['ModelName', 'I/E', 'Truthfullness']].groupby(['I/E', 'ModelName']).mean().reset_index()\n",
    "N_S = df_agents[['ModelName', 'N/S', 'Truthfullness']].groupby(['N/S', 'ModelName']).mean().reset_index()\n",
    "T_F = df_agents[['ModelName', 'T/F', 'Truthfullness']].groupby(['T/F', 'ModelName']).mean().reset_index()\n",
    "J_P = df_agents[['ModelName', 'J/P', 'Truthfullness']].groupby(['J/P', 'ModelName']).mean().reset_index()\n",
    "\n",
    "# Combine tables\n",
    "df_combined = pd.concat([I_E, N_S, T_F, J_P], axis=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Plot for I/E\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='ModelName', y='Truthfullness', hue='I/E', data=df_combined)\n",
    "plt.title('Truthfulness by Model and I/E')\n",
    "plt.xlabel('Model Name')\n",
    "plt.ylabel('Truthfulness')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='I/E')\n",
    "\n",
    "# Plot for N/S\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x='ModelName', y='Truthfullness', hue='N/S', data=df_combined)\n",
    "plt.title('Truthfulness by Model and N/S')\n",
    "plt.xlabel('Model Name')\n",
    "plt.ylabel('Truthfulness')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='N/S')\n",
    "\n",
    "# Plot for T/F\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.barplot(x='ModelName', y='Truthfullness', hue='T/F', data=df_combined)\n",
    "plt.title('Truthfulness by Model and T/F')\n",
    "plt.xlabel('Model Name')\n",
    "plt.ylabel('Truthfulness')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='T/F')\n",
    "\n",
    "# Plot for J/P\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.barplot(x='ModelName', y='Truthfullness', hue='J/P', data=df_combined)\n",
    "plt.title('Truthfulness by Model and J/P')\n",
    "plt.xlabel('Model Name')\n",
    "plt.ylabel('Truthfulness')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='J/P')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_path + \"truthfulness_by_model_and_dichotomies.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "models = df_agents['ModelName'].unique()\n",
    "\n",
    "fig, axs = plt.subplots(len(models), 1, figsize=(15, 5 * len(models)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_data = df_agents[df_agents['ModelName'] == model]\n",
    "    sns.violinplot(x='I/E', y='Truthfullness', data=model_data, ax=axs[i], hue='I/E', split=False)\n",
    "    axs[i].set_title(f'Truthfulness by I/E Dichotomy - {model}')\n",
    "    axs[i].set_xlabel('I/E Dichotomy')\n",
    "    axs[i].set_ylabel('Truthfulness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_path + \"truthfulness_by_IE_per_model.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
